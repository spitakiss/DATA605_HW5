---
title: 'DATA 605: Week 5 Assignment'
author: "Aaron Grzasko"
date: "March 4, 2017"
output: 
    html_document:
        theme: default
        highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA)
```
## Problem Set 1  
  
**In this problem set we'll work out some properties of the least squares solution that we
reviewed in the weekly readings. Consider the unsolvable system $A\vec{x} = \vec{b}$ as given below:**  

$$\begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}\begin{bmatrix} { x }_{ 1 } \\ { x }_{ 2 } \end{bmatrix}=\begin{bmatrix} 0 \\ 8 \\ 8 \\ 20 \end{bmatrix}$$
<br>  

**Write an R markdown script to compute ${A}^{T}A$ and ${A}^{T} \vec{b}$**:  


```{r}

# matrix A
A <- matrix(c(1,1,1,1,0,1,3,4),4,2)
A

# vector b
b <- matrix(c(0,8,8,20))
b

# matrix A transpose
A_t <- t(A)
A_t

# A^T A
A_t.A <- A_t %*% A
A_t.A

# A^T b
A_t.b <- A_t %*% b
A_t.b
```
<br>  

**Solve for $\hat { x }$ in R using the above two computed matrices:**  

Without an explicit solution to $A\vec{x}=\vec{b}$, we use the following equation:  

  
$${ A }^{ T }(\vec{b}-A\hat{x})=0$$

 
With additional algebraic manipulation, we have:  

$${ A }^{ T }A\hat { x } ={ A }^{ T }\vec{b}$$ 

$${  { { (A }^{ T }A })^{ -1 }(A }^{ T }A)\hat { x } = { { (A }^{ T }A })^{ -1 }{ A }^{ T }\vec{b}$$  
$$\hat { x } ={ { (A }^{ T }A })^{ -1 }{ A }^{ T }\vec{b}$$  
```{r}
# solve for x hat:
xhat <- solve(A_t.A) %*% A_t.b
xhat


```
<br>  
  
**What is the squared error of this solution?**  

```{r}

#first, define error vector b - A x_hat  

e <- b - A %*% xhat
e


# calculate squared error as (A x_hat - b)^T (A x_hat - b)  i.e ||A x_hat - b||^2 
sq_error <- t(e) %*% e
sq_error
```
<br>  
  
**Instead of $\vec{b}$ = [0; 8; 8; 20], start with $\vec{p}$ = [1; 5; 13; 17] and find the exact solution (i.e. show that this system is solvable as all equations are consistent with each other. This should result in an error vector $\vec{e} = 0$).**
  
We can use Gauss-Jordan elimination to solve the equation $A\hat{x}=\vec{p}$. We see that the solution is consistent (i.e. there are explicit values of $\hat{x}$ that satisfy the system of equations) and unique.  

```{r}
# initialize vector p
p <- matrix(c(1,5,13,17))

# create augmented matrix A
A_aug <- cbind(A,p)
A_aug

# perform elimination steps, column 1
E21 <- matrix(c(1,0,0,0, -A_aug[2,1]/A_aug[1,1],1,0,0,0,0,1,0,0,0,0,1),4,4,byrow=TRUE) 
E31 <- matrix(c(1,0,0,0,0,1,0,0,-A_aug[3,1]/A_aug[1,1],0,1,0,0,0,0,1),4,4,byrow=TRUE) 
E41 <- matrix(c(1,0,0,0,0,1,0,0,0,0,1,0,-A_aug[4,1]/A_aug[1,1],0,0,1),4,4,byrow=TRUE) 

# initialize R Aug matrix, with elimination steps for column 1
R_aug <- E41 %*% E31 %*% E21 %*% A_aug

# perform elimination steps column two.  
E32 <- matrix(c(1,0,0,0,0,1,0,0,0,-A_aug[3,2]/A_aug[2,2],1,0,0,0,0,1),4,4,byrow=TRUE) 
E42 <- matrix(c(1,0,0,0,0,1,0,0,0,0,1,0,0,-A_aug[4,2]/A_aug[2,2],0,1),4,4,byrow=TRUE) 

# With elimination steps applied to column 2, matrix R_aug is now in reduced row echelon form. 
# solved system: x1 = 1, x2 = 4 
R_aug <- E42 %*% E32  %*% R_aug

R_aug    

# extract solution terms from augmented matrix
x_hat <- matrix(c(R_aug[1,3],R_aug[2,3]))
x_hat
```
<br>  

Let's check that the error term, $\vec{e} = \vec{p}-A\hat{x}=0$:  
```{r}
# e = p - Axhat = 0
e <- p - A %*% x_hat
e

```
<br>  

Instead of using Gauss-Jordan elimination, we can solve for $\hat{x}$ using the following equation:  
$$\hat { x } ={ { (A }^{ T }A })^{ -1 }{ A }^{ T }\vec{p}$$  
This is identical to our earlier equation for $\hat{x}$, but now we've substituted the vector $\vec{p}$ for $\vec{b}$:  


```{r}
x_hat <- solve(A_t.A) %*% A_t %*% p 
x_hat
```
  
Once again, the error term is 0.  
*Note:  the calculation below exhibits small rounding errors.*    

```{r}
# e = p - Axhat = 0
e <- p - A %*% x_hat
e

```


<br>  
**Show that the error $\vec{e}=\vec{b}-\vec{p}$  = [-1; 3, -5, 3]:**   

We calculated this error vector earlier, but we used the term $A\vec{x}$ in place of $\vec{p}$.  
Since $\vec{p} = A\vec{x}$,  the error vector below is identical to our earlier calculation. 

```{r}
e <- b - p 
e
```
<br>  

**Show that the error $\vec{e}$ is orthogonal to $\vec{p}$ and to each of the columns of A.**  

If the dot product of two vectors is zero, then the vectors are orthogonal to each other.   
 
```{r}

# show that the dot product of e and p is 0  
t(e) %*% p


# show that the dot product of e and column 1 of A is 0
t(e) %*% A[,1]

# show that the dot product of e and column 2 of A is 0
t(e) %*% A[,2]


```
  
## Problem Set 2  
  
**Consider the modified auto-mpg data (obtained from the UC Irvine Machine Learning
dataset). This dataset contains 5 columns: *displacement, horsepower, weight, acceleration,
mpg*. We are going to model *mpg* as a function of the other four variables.**  
  
**Write an R markdown script that takes in the auto-mpg data, extracts a matrix, A,
from the first 4 columns and $\vec{b}$ vector from the fifth (*mpg*) column.**  

  
```{r}
# read in data from github url
myurl <- "https://raw.githubusercontent.com/spitakiss/Data605_HW5/master/auto-mpg.data"

car_data <- read.table(myurl, col.names = c("disp","hp","wt","accel","mpg"))

# look at first six rows of data set 
head(car_data)

# summary of data by field
knitr::kable(summary(car_data))

# intercept colummn vectr
intcpt <- rep(1, nrow(car_data)) 

# create matrix A, which includes intercept vector and first four columns of car_data
A <- as.matrix(cbind(intcpt,car_data[,1:4]))
head(A)

# assign column vector b, our dependent variable
b <- as.matrix(car_data[,5]) 
head(b)

```
  
**Using the *least squares* approach, your code should compute the best fitting solution. That is, find the best fitting equation that expresses *mpg* in terms of the other 4 variables.**  
  
Solve for $\hat{x}$ using the following equation:  

$$\hat { x } ={ { (A }^{ T }A })^{ -1 }{ A }^{ T }\vec{b}$$ 

```{r}
# calculate the best-ftting coefficient vector, x_hat
x_hat <- solve(t(A) %*% A) %*% t(A) %*% b  
x_hat
```


**Finally, calculate the fitting error between the predicted mpg of your model and the actual mpg**
```{r}
# error vector
e <- b - A %*% x_hat
head(e)

# squared error
sq_error <- t(e) %*% e
sq_error

```
  
Now, we'll double check our work using built-in r function, `lm()`:  
```{r}
# construct linear model from original car data set
my_lm <- lm(mpg ~ disp + hp + wt + accel, data = car_data)
my_lm
```

Finally let's verify that the fitted coefficients in lm() are identical to our manually calculated coefficients:  

```{r}

# check intercept
abs(my_lm$coefficients["(Intercept)"] - x_hat["intcpt",1]) < 0.00001

# check disp coefficient
abs(my_lm$coefficients["disp"] - x_hat["disp",1]) < 0.00001

# check hp coefficient
abs(my_lm$coefficients["hp"] - x_hat["hp",1]) < 0.00001

# check wt coefficient
abs(my_lm$coefficients["wt"] - x_hat["wt",1]) < 0.00001

# check accel coefficient
abs(my_lm$coefficients["accel"] - x_hat["accel",1]) < 0.00001




```



  


